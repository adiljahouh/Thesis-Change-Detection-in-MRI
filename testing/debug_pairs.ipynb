{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available()) # should be True\n",
    "# t = torch.rand(10, 10).cuda()\n",
    "# print(t.device) # should be CUDA]\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import torchio as tio\n",
    "from typing import List, Tuple\n",
    "from numpy import ndarray\n",
    "from nilearn.image import resample_to_img\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torchio as tio\n",
    "from nilearn.image import resample_to_img\n",
    "from numpy import ndarray\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def various_distance(out_vec_t0, out_vec_t1,dist_flag):\n",
    "    if dist_flag == 'l2':\n",
    "        distance = F.pairwise_distance(out_vec_t0, out_vec_t1, p=2)\n",
    "    if dist_flag == 'l1':\n",
    "        distance = F.pairwise_distance(out_vec_t0, out_vec_t1, p=1)\n",
    "    if dist_flag == 'cos':\n",
    "        distance = 1 - F.cosine_similarity(out_vec_t0, out_vec_t1)\n",
    "    return distance\n",
    "\n",
    "import random\n",
    "\n",
    "def balance_dataset(subject_images, label_key='label'):\n",
    "    \"\"\"\n",
    "    Balances the dataset by undersampling the majority class to match the size of the minority class.\n",
    "    \n",
    "    Parameters:\n",
    "    subject_images (list): List of dictionaries containing images and labels.\n",
    "    label_key (str): Key used to access the label in the dictionaries.\n",
    "    \n",
    "    Returns:\n",
    "    list: Balanced list of dictionaries.\n",
    "    \"\"\"\n",
    "\n",
    "    similar_pairs = [x for x in subject_images if x[label_key] == 1]\n",
    "    dissimilar_pairs = [x for x in subject_images if x[label_key] == 0]\n",
    "    num_similar_pairs = len(similar_pairs)\n",
    "    num_dissimilar_pairs = len(dissimilar_pairs)\n",
    "\n",
    "    if num_similar_pairs > num_dissimilar_pairs:\n",
    "        similar_pairs = random.sample(similar_pairs, num_dissimilar_pairs)\n",
    "    else:\n",
    "        dissimilar_pairs = random.sample(dissimilar_pairs, num_similar_pairs)\n",
    "    balanced_subject_images = similar_pairs + dissimilar_pairs\n",
    "    random.shuffle(balanced_subject_images)  # Shuffle to mix the pairs\n",
    "    return balanced_subject_images\n",
    "\n",
    "def stratified_kfold_split(dataset, n_splits=5):\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    splits = list(skf.split(range(len(dataset)), dataset.labels))\n",
    "    return splits\n",
    "\n",
    "def normalize_nifti(nifti_image: nib.Nifti1Image) -> ndarray:\n",
    "    return (nifti_image.get_fdata() - np.min(nifti_image.get_fdata())) / (np.max(\n",
    "        nifti_image.get_fdata()) - np.min(nifti_image.get_fdata()))\n",
    "\n",
    "def pad_slice(slice_2d: ndarray, output_size=(256, 256)) -> ndarray:\n",
    "    \"\"\"\n",
    "    Pad a 2D slice to the desired output size with zeros.\n",
    "    \"\"\"\n",
    "    pad_height = (output_size[0] - slice_2d.shape[0])\n",
    "    pad_width = (output_size[1] - slice_2d.shape[1])\n",
    "    \n",
    "    padded_slice = np.pad(slice_2d, \n",
    "                          ((0, pad_height), (0, pad_width)), \n",
    "                          mode='constant', \n",
    "                          constant_values=0)\n",
    "    return padded_slice\n",
    "\n",
    "def slice_has_high_info(slice_2d: np.ndarray, value_minimum=0.15, percentage_minimum=0.05):\n",
    "    ## checks if the slice has high information by a certain value threshold and percentage of cells\n",
    "    total_cells = slice_2d.size\n",
    "    num_high_info_cells = np.count_nonzero(slice_2d >= value_minimum)\n",
    "    percentage_high_info = num_high_info_cells / total_cells\n",
    "    return percentage_high_info > percentage_minimum\n",
    "\n",
    "def balance_classes_slices():\n",
    "    return\n",
    "\n",
    "def convert_3d_into_2d(nifti_image: ndarray, skip: int =1) -> list[Tuple[ndarray, Tuple[int, int, int]]]:\n",
    "    slices = []\n",
    "   \n",
    "    # (axial)\n",
    "    ## TODO: Use all slices for now only using every 4th slices\n",
    "    for i in range(nifti_image.shape[0]):\n",
    "        if i % skip == 0:\n",
    "            slices.append((nifti_image[i, :, :], (i, -1 , -1)))\n",
    "    #  (coronal)\n",
    "    for i in range(nifti_image.shape[1]):\n",
    "        if i % skip == 0:\n",
    "            slices.append((nifti_image[:, i, :], (-1, i, -1)))  \n",
    "    # (sagittal)\n",
    "    for i in range(nifti_image.shape[2]):\n",
    "        if i % skip == 0:\n",
    "            slices.append((nifti_image[:, :, i], (-1, -1, i)))\n",
    "    return slices\n",
    "\n",
    "def has_tumor_cells(slice_2d: ndarray, threshold=0.15):\n",
    "    ## checks if the slice has tumor cells by a certain value threshold\n",
    "    return np.any(slice_2d >= threshold)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class imagePairs(Dataset):\n",
    "    \"\"\"\n",
    "    Image dataset for each subject in the dataset\n",
    "    creating only 'correct' and 'incorrect' pairs for now\n",
    "\n",
    "    Works by passing preop or postop directory to the class\n",
    "    and finds the corresponding image in the other dir and labels\n",
    "    \"\"\"\n",
    "    def __init__(self, proc_preop: str, raw_tumor_dir: str, image_ids: list, transform=None, skip:int=1,\n",
    "                 tumor_sensitivity = 0.10):\n",
    "        self.root = proc_preop\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = [] # used for kfold later on\n",
    "        self.image_ids = image_ids\n",
    "        for root, dirs, files in os.walk(self.root):\n",
    "            for filename in files:\n",
    "                for image_id in self.image_ids:\n",
    "                    if filename.endswith(image_id):\n",
    "                        try:\n",
    "                            pat_id = root.split(\"/\")[-1]\n",
    "                            print(f\"Processing {pat_id}\")\n",
    "                            preop_nifti = nib.load(os.path.join(root, filename))\n",
    "                            postop_nifti = nib.load(os.path.join(root.replace(\"preop\", \"postop\"), \n",
    "                                                                filename.replace(\"preop\", \"postop\")))\n",
    "                            # print(preop_nifti.shape)\n",
    "                            # print(postop_nifti.shape)\n",
    "                            # load the tumor from the tumor directory matching the patient id\n",
    "                            if \"PAT\" in pat_id:\n",
    "                                try:\n",
    "                                    tumor = nib.load(os.path.join(f\"{raw_tumor_dir}/{pat_id}/anat/{pat_id}_space_T1_label-tumor.nii\"))\n",
    "                                    tumor_resampled = resample_to_img(tumor, preop_nifti, interpolation='nearest')\n",
    "                                    tumor_norm = normalize_nifti(tumor_resampled)\n",
    "                                except FileNotFoundError as e:\n",
    "                                    print(f\"Tumor not found for {pat_id}, {e}\")\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Uncaught error, {e}\")\n",
    "                            \n",
    "                            # resample the postop nifti to the preop nifti\n",
    "                            preop_nifti_norm = normalize_nifti(preop_nifti)\n",
    "                            postop_nifti_norm = normalize_nifti(postop_nifti)\n",
    "\n",
    "                            if \"-CON\" in pat_id:\n",
    "                                assert preop_nifti_norm.shape == postop_nifti_norm.shape\n",
    "                                \n",
    "                                images_pre = convert_3d_into_2d(preop_nifti_norm, skip=skip)\n",
    "                                images_post = convert_3d_into_2d(postop_nifti_norm, skip = skip)\n",
    "\n",
    "                                # Create triplets with label 1 (similar slices)\n",
    "                                images_pre_pad = [(pad_slice(image[0]), image[1], 1) for image in images_pre]\n",
    "                                images_post_pad = [(pad_slice(image[0]), image[1], 1) for image in images_post]\n",
    "                                \n",
    "                                assert len(images_pre_pad) == len(images_post_pad)\n",
    "                                \n",
    "                                # Create triplets (pre_slice, post_slice, label, tumor=None)\n",
    "                                triplets_con = [{\"pre\": pre, \"post\": post, \"label\": label, \"tumor\": np.zeros_like(pre), \n",
    "                                                 \"pat_id\": pat_id, \"index_pre\": index_pre, \"index_post\": index_post} \n",
    "                                                for (pre, index_pre, label), \n",
    "                                                (post, index_post, _) in \n",
    "                                                zip(images_pre_pad, images_post_pad) if \n",
    "                                                slice_has_high_info(pre) and slice_has_high_info(post)]\n",
    "                                self.data.extend(triplets_con)\n",
    "                                self.labels.extend([label for (_, label, _) in images_post_pad])\n",
    "                            elif \"-PAT\" in pat_id:\n",
    "                                assert preop_nifti_norm.shape == postop_nifti_norm.shape == tumor_norm.shape\n",
    "\n",
    "                                images_pre = convert_3d_into_2d(preop_nifti_norm, skip=skip)\n",
    "                                images_post = convert_3d_into_2d(postop_nifti_norm, skip=skip)\n",
    "                                mask_slices = convert_3d_into_2d(tumor_norm, skip=skip)\n",
    "                                # Create triplets with label 0 if the slice contains a tumor\n",
    "                                images_pre_pad = [(pad_slice(image[0]), image[1], 0 if has_tumor_cells(mask_slice[0], threshold=tumor_sensitivity) else 1) for image, mask_slice in zip(images_pre, mask_slices)]\n",
    "                                images_post_pad = [(pad_slice(image[0]), image[1], 0 if has_tumor_cells(mask_slice[0], threshold=tumor_sensitivity) else 1) for image, mask_slice in zip(images_post, mask_slices)]\n",
    "                                # pad the tumor mask as well\n",
    "                                mask_slices_pad = [(pad_slice(mask_slice[0]), mask_slice[1]) for mask_slice in mask_slices]\n",
    "                                assert len(images_pre_pad) == len(images_post_pad) == len(mask_slices_pad)\n",
    "                                \n",
    "                                # Create triplets (pre_slice, post_slice, label, tumor)\n",
    "                                triplets_pat = [{\"pre\": pre, \"post\": post, \"label\": label, \"tumor\": mask_slice, \n",
    "                                                 \"pat_id\": pat_id, \"index_pre\": index_pre, \"index_post\": index_post} \n",
    "                                                for (pre, index_pre, label), (post, index_post, _), (mask_slice, _) in \n",
    "                                                zip(images_pre_pad, images_post_pad, mask_slices_pad) if \n",
    "                                                slice_has_high_info(pre) and slice_has_high_info(post)]\n",
    "                                 \n",
    "                                self.data.extend(triplets_pat)\n",
    "                                self.labels.extend([label for (_, label, _) in images_post_pad])\n",
    "                        except FileNotFoundError as e:\n",
    "                            print(f\"{e}, this is normal to happen for 3 subjects which have no postoperative data\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Uncaught error, {e}\")\n",
    "                    else:\n",
    "                        pass\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.transform:\n",
    "            pass\n",
    "            # img1_file = self.transform(self.data[idx][0])\n",
    "            # img2_file = self.transform(self.data[idx][1])\n",
    "        return self.data[idx]\n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subject_pairs(root, id):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(root):\n",
    "        for filename in files:\n",
    "            for image_id in id:\n",
    "                if filename.endswith(image_id):\n",
    "                    nifti_1 = tio.ScalarImage(os.path.join(root, filename))\n",
    "                    try:\n",
    "                        if \"preop\" in root:\n",
    "                            nifti_2 = tio.ScalarImage(os.path.join(root.replace(\"preop\", \"postop\"), filename.replace(\"preop\", \"postop\")))\n",
    "                        else:\n",
    "                            nifti_2 = tio.ScalarImage(os.path.join(root.replace(\"postop\", \"preop\"), filename.replace(\"postop\", \"preop\")))\n",
    "                        if \"-CON\" in filename or \"-CON\" in os.path.join(root, filename):\n",
    "                            # print(\"control for \", filename)\n",
    "                            data.append(\n",
    "                                tio.Subject(\n",
    "                                    t1=nifti_1,\n",
    "                                    t2=nifti_2,\n",
    "                                    label=1,\n",
    "                                    name= root.split(\"/\")[-1],\n",
    "                                    path= os.path.join(root, filename)\n",
    "                                            )\n",
    "                                    )\n",
    "                        elif \"-PAT\" in filename or \"-PAT\" in os.path.join(root, filename):\n",
    "                                data.append(\n",
    "                                tio.Subject(\n",
    "                                    t1=nifti_1,\n",
    "                                    t2=nifti_2,\n",
    "                                    label=0,\n",
    "                                    name= root.split(\"/\")[-1],\n",
    "                                    path= os.path.join(root, filename)\n",
    "                                            )\n",
    "                                    )\n",
    "                        else:\n",
    "                            print(f\"Invalid filename: {os.path.join(root, filename)}\")\n",
    "                    except FileNotFoundError:\n",
    "                        print(f\"Matching subject (pre and post) not found for {os.path.join(root, filename)}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_subjects(subjects: list[tio.Subject]) -> tio.SubjectsDataset:\n",
    "    transforms = [\n",
    "    # tio.RescaleIntensity(out_min_max=(0, 1)),\n",
    "    tio.CropOrPad((164, 164, 164)),\n",
    "    ]\n",
    "    transform = tio.Compose(transforms)\n",
    "    return tio.SubjectsDataset(subjects, transform=transform)\n",
    "\n",
    "def create_loaders_with_index(dataset, train_index, test_index, batch_size=1):\n",
    "    train_dataset = Subset(dataset, train_index)\n",
    "    test_dataset = Subset(dataset, test_index)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def create_loaders_with_split(dataset: Dataset, split=(0.8, 0.2), generator=None):\n",
    "    train_t1, test_t1 = random_split(dataset=dataset, lengths=split, generator=generator)\n",
    "    BATCH_SIZE=1\n",
    "    train_loader_t1 = DataLoader(train_t1, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader_t1 = DataLoader(test_t1, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    return train_loader_t1, test_loader_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_images = imagePairs(proc_preop='./data/processed/preop/BTC-preop', \n",
    "            raw_tumor_dir='./data/raw/preop/BTC-preop/derivatives/tumor_masks',\n",
    "            image_ids=['t1_ants_aligned.nii.gz'], skip=2, tumor_sensitivity=0.15)\n",
    "# balance subject_images based on label\n",
    "print(f\"Total number of images: {len(subject_images)}\")\n",
    "print(\"Number of similar pairs:\", len([x for x in subject_images if x['label'] == 1]))\n",
    "print(\"Number of dissimilar pairs:\", len([x for x in subject_images if x['label'] == 0]))\n",
    "\n",
    "subject_images = balance_dataset(subject_images)\n",
    "print(f\"Total number of images after balancing: {len(subject_images)}\")\n",
    "train_subject_images, val_subject_images, test_subject_images = random_split(subject_images, (0.6, 0.2, 0.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
