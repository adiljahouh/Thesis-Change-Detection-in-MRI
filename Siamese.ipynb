{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change detection (invariant to angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available()) # should be True\n",
    "# t = torch.rand(10, 10).cuda()\n",
    "# print(t.device) # should be CUDA\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import nibabel as nib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with saggital midpoint images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_image_if_not_exist(directory, output_size=(256, 256)):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\"nii_mask.nii.gz\"):\n",
    "                # Construct input and output paths\n",
    "                input_path = os.path.join(root, filename)\n",
    "                output_path = os.path.join(root, os.path.splitext(filename)[0] + \"saggital_view\" + \".jpg\")\n",
    "\n",
    "                # Check if output image already exists\n",
    "                if os.path.exists(output_path):\n",
    "                    print(f\"Image {output_path} already exists, skipping...\") \n",
    "                    continue\n",
    "                # Saggital - 0, Coronal - 1, Axial - 2\n",
    "                # Load NIfTI data\n",
    "                nifti_data = nib.load(input_path)\n",
    "                image_data = nifti_data.get_fdata()\n",
    "\n",
    "                slice_index = image_data.shape[0] // 2\n",
    "                image_slice = image_data[slice_index, :, :]\n",
    "\n",
    "                # Normalize intensity values\n",
    "                min_intensity = np.min(image_slice)\n",
    "                max_intensity = np.max(image_slice)\n",
    "                image_slice_normalized = (image_slice - min_intensity) / (max_intensity - min_intensity)\n",
    "\n",
    "                # Resize the slice to the specified output size\n",
    "                image_slice_resized = np.array(Image.fromarray((image_slice_normalized * 255).astype(np.uint8)).resize(output_size))\n",
    "\n",
    "                # Convert to image format\n",
    "                image = Image.fromarray(image_slice_resized)\n",
    "\n",
    "                # Resize the image to the specified output size\n",
    "                image = image.resize(output_size)\n",
    "\n",
    "                # Save the image\n",
    "                image.save(output_path)\n",
    "                print(f\"Converted masked image {input_path} to {output_path} with size {output_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image ./data/raw/preop/BTC-preop/sub-PAT31/ses-preop/anat/sub-PAT31_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-CON09/ses-preop/anat/sub-CON09_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-CON03/ses-preop/anat/sub-CON03_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT25/ses-preop/anat/sub-PAT25_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT14/ses-preop/anat/sub-PAT14_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT05/ses-preop/anat/sub-PAT05_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-CON02/ses-preop/anat/sub-CON02_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT26/ses-preop/anat/sub-PAT26_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT16/ses-preop/anat/sub-PAT16_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT01/ses-preop/anat/sub-PAT01_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT24/ses-preop/anat/sub-PAT24_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT13/ses-preop/anat/sub-PAT13_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-CON06/ses-preop/anat/sub-CON06_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-CON01/ses-preop/anat/sub-CON01_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT02/ses-preop/anat/sub-PAT02_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-CON08/ses-preop/anat/sub-CON08_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT10/ses-preop/anat/sub-PAT10_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT19/ses-preop/anat/sub-PAT19_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-CON04/ses-preop/anat/sub-CON04_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT17/ses-preop/anat/sub-PAT17_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT27/ses-preop/anat/sub-PAT27_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT07/ses-preop/anat/sub-PAT07_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-CON11/ses-preop/anat/sub-CON11_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-CON07/ses-preop/anat/sub-CON07_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT15/ses-preop/anat/sub-PAT15_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT29/ses-preop/anat/sub-PAT29_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT08/ses-preop/anat/sub-PAT08_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-CON10/ses-preop/anat/sub-CON10_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT23/ses-preop/anat/sub-PAT23_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT20/ses-preop/anat/sub-PAT20_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT22/ses-preop/anat/sub-PAT22_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT06/ses-preop/anat/sub-PAT06_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-CON05/ses-preop/anat/sub-CON05_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT03/ses-preop/anat/sub-PAT03_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT11/ses-preop/anat/sub-PAT11_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/preop/BTC-preop/sub-PAT28/ses-preop/anat/sub-PAT28_ses-preop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n"
     ]
    }
   ],
   "source": [
    "dir = \"./data/raw/preop/BTC-preop\"\n",
    "output_size = (256, 256)  # Specify the desired output size\n",
    "convert_to_image_if_not_exist(dir, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image ./data/raw/postop/BTC-postop/sub-CON09/ses-postop/anat/sub-CON09_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-CON03/ses-postop/anat/sub-CON03_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT25/ses-postop/anat/sub-PAT25_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT05/ses-postop/anat/sub-PAT05_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-CON02/ses-postop/anat/sub-CON02_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT26/ses-postop/anat/sub-PAT26_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT16/ses-postop/anat/sub-PAT16_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT01/ses-postop/anat/sub-PAT01_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT24/ses-postop/anat/sub-PAT24_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT13/ses-postop/anat/sub-PAT13_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-CON06/ses-postop/anat/sub-CON06_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT02/ses-postop/anat/sub-PAT02_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-CON08/ses-postop/anat/sub-CON08_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT10/ses-postop/anat/sub-PAT10_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-CON04/ses-postop/anat/sub-CON04_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT17/ses-postop/anat/sub-PAT17_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT07/ses-postop/anat/sub-PAT07_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-CON11/ses-postop/anat/sub-CON11_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-CON07/ses-postop/anat/sub-CON07_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT15/ses-postop/anat/sub-PAT15_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT08/ses-postop/anat/sub-PAT08_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-CON10/ses-postop/anat/sub-CON10_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT23/ses-postop/anat/sub-PAT23_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT20/ses-postop/anat/sub-PAT20_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT06/ses-postop/anat/sub-PAT06_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-CON05/ses-postop/anat/sub-CON05_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT03/ses-postop/anat/sub-PAT03_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT11/ses-postop/anat/sub-PAT11_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n",
      "Image ./data/raw/postop/BTC-postop/sub-PAT28/ses-postop/anat/sub-PAT28_ses-postop_T1w.nii_mask.niisaggital_view.jpg already exists, skipping...\n"
     ]
    }
   ],
   "source": [
    "dir = \"./data/raw/postop/BTC-postop\"\n",
    "output_size = (256, 256)  # Specify the desired output size\n",
    "convert_to_image_if_not_exist(dir, output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class imageSets(Dataset):\n",
    "    \"\"\"\n",
    "    Image dataset for each subject in the dataset\n",
    "    creating only 'correct' pairs for now\n",
    "    TODO: create 'incorrect' pairs\n",
    "\n",
    "    Works by passing preop or postop directory to the class\n",
    "    and finds the corresponding image in the other dir and labels\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        for root, dirs, files in os.walk(self.root):\n",
    "            for filename in files:\n",
    "                if filename.endswith(\"saggital_view.jpg\"):\n",
    "                    img_1 = Image.open(os.path.join(root, filename))\n",
    "                    ## finds the corresponding image in the other dir\n",
    "                    try:\n",
    "                        if \"preop\" in root:\n",
    "                            img_2 = Image.open(os.path.join(root.replace(\"preop\", \"postop\"), filename.replace(\"preop\", \"postop\")))\n",
    "                        else:\n",
    "                            img_2 = Image.open(os.path.join(root.replace(\"postop\", \"preop\"), filename.replace(\"postop\", \"preop\")))\n",
    "                        self.data.append((img_1, img_2, 1))\n",
    "                    except FileNotFoundError:\n",
    "                        print(f\"Matching subject (pre and post) not found for {filename}\")\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform:\n",
    "            img1_file = self.transform(self.data[idx][0])\n",
    "            img2_file = self.transform(self.data[idx][1])\n",
    "        return (img1_file, img2_file, self.data[idx][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # Define the architecture for the Siamese network\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(131072, 128)  # Adjust input size based on input dimensions\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # Forward pass through the Siamese network\n",
    "        output1 = F.relu(self.bn1(self.conv1(input1)))\n",
    "        output1 = F.max_pool2d(output1, kernel_size=2, stride=2)\n",
    "        output1 = F.relu(self.bn2(self.conv2(output1)))\n",
    "        output1 = F.max_pool2d(output1, kernel_size=2, stride=2)\n",
    "        output1 = F.relu(self.bn3(self.conv3(output1)))\n",
    "        output1 = F.max_pool2d(output1, kernel_size=2, stride=2)\n",
    "        output1 = output1.view(output1.size(0), -1)\n",
    "        output1 = self.dropout(output1)\n",
    "        output1 = F.relu(self.fc1(output1))\n",
    "\n",
    "        output2 = F.relu(self.bn1(self.conv1(input2)))\n",
    "        output2 = F.max_pool2d(output2, kernel_size=2, stride=2)\n",
    "        output2 = F.relu(self.bn2(self.conv2(output2)))\n",
    "        output2 = F.max_pool2d(output2, kernel_size=2, stride=2)\n",
    "        output2 = F.relu(self.bn3(self.conv3(output2)))\n",
    "        output2 = F.max_pool2d(output2, kernel_size=2, stride=2)\n",
    "        output2 = output2.view(output2.size(0), -1)\n",
    "        output2 = self.dropout(output2)\n",
    "        output2 = F.relu(self.fc1(output2))\n",
    "\n",
    "        return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, input1, input2, y):\n",
    "        diff = input1 - input2\n",
    "        dist_sq = torch.sum(torch.pow(diff, 2), 1)\n",
    "        dist = torch.sqrt(dist_sq)\n",
    "        mdist = self.margin - dist\n",
    "        dist = torch.clamp(mdist, min=0.0)\n",
    "        loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n",
    "        loss = torch.sum(loss) / 2.0 / input1.size()[0]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Siamese network architecture\n",
    "\n",
    "\n",
    "# Train the Siamese network\n",
    "def train(siamese_net,  optimizer, criterion, epochs=10):\n",
    "    default_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "    BATCH_SIZE = 4\n",
    "    train_data = imageSets(\"./data/raw/preop/BTC-preop\", transform=default_transform)\n",
    "    loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    siamese_net.train()\n",
    "    \n",
    "    print(\"starting training...\")\n",
    "    total_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0000\n",
    "        for i, (img1_set, img2_set, label) in enumerate(loader):\n",
    "            # img1_set = img1_set.cuda()\n",
    "            # img2_set = img2_set.cuda()\n",
    "            # label = label.cuda()\n",
    "            print(label)\n",
    "\n",
    "            output1, output2 = siamese_net(img1_set, img2_set)\n",
    "            loss = criterion(output1, output2, label)  # Add unsqueeze to match output shape\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(loss.item())\n",
    "            epoch_loss += loss.item()\n",
    "           #test\n",
    "        print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, epochs, epoch_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found for sub-PAT31_ses-preop_T1w.nii_mask.niisaggital_view.jpg\n",
      "File not found for sub-PAT14_ses-preop_T1w.nii_mask.niisaggital_view.jpg\n",
      "File not found for sub-CON01_ses-preop_T1w.nii_mask.niisaggital_view.jpg\n",
      "File not found for sub-PAT19_ses-preop_T1w.nii_mask.niisaggital_view.jpg\n",
      "File not found for sub-PAT27_ses-preop_T1w.nii_mask.niisaggital_view.jpg\n",
      "File not found for sub-PAT29_ses-preop_T1w.nii_mask.niisaggital_view.jpg\n",
      "File not found for sub-PAT22_ses-preop_T1w.nii_mask.niisaggital_view.jpg\n",
      "starting training...\n",
      "tensor([1, 1, 1, 1])\n",
      "20.54092025756836\n",
      "tensor([1, 1, 1, 1])\n",
      "16.242149353027344\n",
      "tensor([1, 1, 1, 1])\n",
      "5.703119277954102\n",
      "tensor([1, 1, 1, 1])\n",
      "2.217747926712036\n",
      "tensor([1, 1, 1, 1])\n",
      "2.8933587074279785\n",
      "tensor([1, 1, 1, 1])\n",
      "0.0\n",
      "tensor([1, 1, 1, 1])\n",
      "0.0\n",
      "tensor([1])\n",
      "0.0\n",
      "Epoch [1/10], Loss: 47.5973\n",
      "tensor([1, 1, 1, 1])\n",
      "0.0\n",
      "tensor([1, 1, 1, 1])\n",
      "0.0\n",
      "tensor([1, 1, 1, 1])\n",
      "0.0\n",
      "tensor([1, 1, 1, 1])\n",
      "0.0\n",
      "tensor([1, 1, 1, 1])\n",
      "0.0\n",
      "tensor([1, 1, 1, 1])\n",
      "0.0\n",
      "tensor([1, 1, 1, 1])\n",
      "0.0\n",
      "tensor([1])\n",
      "0.0\n",
      "Epoch [2/10], Loss: 0.0000\n",
      "tensor([1, 1, 1, 1])\n",
      "0.0\n",
      "tensor([1, 1, 1, 1])\n",
      "0.0\n",
      "tensor([1, 1, 1, 1])\n",
      "0.0\n",
      "tensor([1, 1, 1, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(siamese_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the Siamese network\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msiamese_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(siamese_net, optimizer, criterion, epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m output1, output2 \u001b[38;5;241m=\u001b[39m siamese_net(img1_set, img2_set)\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output1, output2, label)  \u001b[38;5;66;03m# Add unsqueeze to match output shape\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/Documents/TUE/preparationPhase/myProject/.conda/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/TUE/preparationPhase/myProject/.conda/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize Siamese network\n",
    "siamese_net = SiameseNetwork()\n",
    "# siamese_net = siamese_net.cuda()  # Move the network to GPU\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(siamese_net.parameters(), lr=0.001)\n",
    "\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(siamese_net.parameters(), lr=0.001)\n",
    "# Train the Siamese network\n",
    "train(siamese_net,  optimizer, criterion, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_preop = nib.load(\"./data/raw/preop/sub-CON02_ses-preop_T1w.nii.gz\")\n",
    "# raw_postop = nib.load(\"./data/raw/postop/sub-CON02_ses-postop_T1w.nii.gz\")\n",
    "\n",
    "# proc_preop = nib.load(\"./data/processed/preop/fa.nii.gz\")\n",
    "# proc_postop = nib.load(\"./data/processed/postop/fa.nii.gz\")\n",
    "\n",
    "# # This apparently returns voxel level of the data\n",
    "# data_raw_preop= raw_preop.get_fdata()\n",
    "# data_raw_postop= raw_postop.get_fdata()\n",
    "\n",
    "# data_proc_preop= proc_preop.get_fdata()\n",
    "# data_proc_postop= proc_postop.get_fdata()\n",
    "\n",
    "# print(data_raw_preop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20693/2275318329.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  raw_pairs_tensor = torch.tensor(raw_pairs, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# def make_pairs():\n",
    "#     pass\n",
    "import sys\n",
    "\n",
    "mock_pairs = [(data_raw_preop, data_raw_postop), (data_raw_preop, data_raw_preop), (data_raw_postop, data_raw_postop)]\n",
    "mock_labels = [1, 0, 0]\n",
    "\n",
    "raw_pairs = mock_pairs\n",
    "raw_labels = mock_labels\n",
    "# Convert the processed data into PyTorch tensors\n",
    "raw_pairs_tensor = torch.tensor(raw_pairs, dtype=torch.float32)\n",
    "raw_labels_tensor = torch.tensor(raw_labels, dtype=torch.float32)\n",
    "print(sys.getsizeof(raw_pairs_tensor))\n",
    "# print(raw_pairs_tensor)\n",
    "\n",
    "# Create DataLoader for training\n",
    "# raw_dataset = TensorDataset(raw_pairs_tensor, raw_labels_tensor)\n",
    "# raw_loader = DataLoader(raw_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseNetwork(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "#         self.subnetwork = SubNetwork(input_size, hidden_size, output_size)\n",
    "    \n",
    "#     def forward(self, input1, input2):\n",
    "#         # Pass inputs through the subnetwork\n",
    "#         output1 = self.subnetwork(input1)\n",
    "#         output2 = self.subnetwork(input2)\n",
    "        \n",
    "#         # Compute the Euclidean distance between the outputs\n",
    "#         distance = torch.sqrt(torch.sum(torch.pow(output1 - output2, 2), dim=1))\n",
    "        \n",
    "#         # Normalize the distance to [0, 1] range\n",
    "#         distance = torch.sigmoid(distance)\n",
    "        \n",
    "#         return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # Define the architecture for the Siamese network\n",
    "        self.fc1 = nn.Linear(256*256, 128)  # Adjust input size based on input dimensions\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)  # Output size 1 for binary classification\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # Flatten the input tensors\n",
    "        input1 = input1.view(input1.size(0), -1)\n",
    "        input2 = input2.view(input2.size(0), -1)\n",
    "        # Forward pass through the Siamese network\n",
    "        output1 = F.relu(self.fc1(input1))\n",
    "        output1 = F.relu(self.fc2(output1))\n",
    "        output2 = F.relu(self.fc1(input2))\n",
    "        output2 = F.relu(self.fc2(output2))\n",
    "        return output1, output2\n",
    "\n",
    "# Initialize Siamese network\n",
    "siamese_net = SiameseNetwork()\n",
    "siamese_net = siamese_net.cuda()  # Move the network to GPU\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(siamese_net.parameters(), lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
